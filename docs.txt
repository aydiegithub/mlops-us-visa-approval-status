## problem statement
# USA visa approval status
- given certain set of features such as continent, education, job-experience, training, employment, current age, etc.
- we have to predict the probability of a visa approval.



# tools and tech
- Python, MongoDB, Flask, Scikit-learn, Pandas, NumPy, Matplotlib, Seaborn



# Solution scope
- This can be used on real life by US visa applicants so that they can improve their resume and criteria to get visa approved.



# Solution Approach
- Machine Learning (classification algorithms)



# Solution Proposed
- Load the data from MongoDB
- Perform EDA and feature engineering to select the desirable features
- Fit the ML classification algorithm and find out the best model
- Select top few and tune the hyperparameters
- Select the best model based on desired metrics



# Project Setup
- GitHub Repository
- Requirements
- Template



# Resources:
- MLOps tool: https://www.evidentlyai.com/
- MongoDB: https://www.mongodb.com/


Step 2
# Setup Database
# Logging Module
# Exception Module
# Utility Module

# From keggle to MongoDB connect it
    - Do data ingestion on DB

# Create the MDB Account
Dataset: https://www.kaggle.com/datasets/moro23/easyvisa-dataset

Created custom exception, custom logger, custom utility functions

Next step:
I am doing EDA, Feature engineering, model training on Jupyter Notebook

Next Step:
We implement data ingestion
 - Data Drift with evidentlyai (mlops tool)
 - Understanding Data Validation, Data Transformation, Model Training, Model Evaluation, Model Pusher
 - Model Training
 - Model Evaluation
 - Model Pusher

Creating a Pipeline
  > Component 1 Data ingestion -> connect to my mongodb using connection string -> <- fetch the data

## Workflow of Pipeline
  1. constants create ( any changes must be made in this folder to reflect in everything automatically)
  2. entity create
  3. components create
  4. pipelin update

    - how artifacts created:
        constants -> cinfig entity -> Data Ingestion -> (Train.parquet, Test.parquet) Artifacts
    - after ingestuon:
        (Train.parquet, Test.parquet) Artifacts -> Data Validation -> Data Transformation -> Model Trainer -> Model Evaluation -> Model Pusher

## Develop all the components
- Data Ingestion developed succesfull for training Pipeline

## Data Drift with evidentlyai amd data Validation
We will solve this by data Transformation then Evaluation

